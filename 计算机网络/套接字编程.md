- [1.地址结构](#1地址结构)
  - [地址转换函数](#地址转换函数)
  - [IPv4对应的结构](#ipv4对应的结构)
  - [IPv6对应的结构](#ipv6对应的结构)
- [2.相关函数](#2相关函数)
  - [socket](#socket)
  - [bind](#bind)
  - [listen](#listen)
  - [accept](#accept)
  - [connect, 连接套接字](#connect-连接套接字)
  - [数据传输函数](#数据传输函数)
  - [关闭套接字](#关闭套接字)
  - [获取源IP、源端口号、目的IP、目的端口号的方法](#获取源ip源端口号目的ip目的端口号的方法)
- [3.TCP通信](#3tcp通信)
- [4.UDP通信](#4udp通信)
  - [缓冲区](#缓冲区)
  - [数据传输](#数据传输)
  - [UDP调用connect](#udp调用connect)
- [5.IO模型](#5io模型)
  - [阻塞和非阻塞I/O区别](#阻塞和非阻塞io区别)
  - [同步和异步区别](#同步和异步区别)
  - [常见的5种](#常见的5种)
- [6.IO复用](#6io复用)
  - [select](#select)
  - [poll](#poll)
  - [select和poll对比](#select和poll对比)
  - [epoll](#epoll)
    - [1.创建epoll文件描述符](#1创建epoll文件描述符)
    - [2. 注册事件](#2-注册事件)
    - [3. 等待事件](#3-等待事件)
    - [EPOLLONESHOT事件](#epolloneshot事件)
    - [LT和ET](#lt和et)
    - [epoll的优缺点：](#epoll的优缺点)
      - [机制实现](#机制实现)
  - [三者对比](#三者对比)
  - [epoll和select的区别，epoll为什么高效](#epoll和select的区别epoll为什么高效)
- [套接字编程常见的异常行为](#套接字编程常见的异常行为)
  - [SIGPIPE](#sigpipe)
  - [解决对端崩溃问题](#解决对端崩溃问题)
- [客户/服务器程序设计范式](#客户服务器程序设计范式)
  - [单进程](#单进程)
  - [多进程](#多进程)
  - [多线程](#多线程)
  - [Reactor模型变种](#reactor模型变种)
  - [Reactors](#reactors)
  - [总结](#总结)

# 1.地址结构

## 地址转换函数

- **主机字节序与网络字节序的转换**
- 一般用于端口转换
  
```cpp
#include <netinet/in.h>

/***********************************************
 * 主机字节序 -> 网络字节序
 ***********************************************/
uint16_t htons(uint16_t host16bitvalue);
uint32_t htonl(uint32_t host32bitvalue);

/***********************************************
 * 网络字节序 -> 主机字节序
 ***********************************************/
uint16_t ntohs(uint16_t net16bitvalue);
uint32_t ntohl(uint32_t net32bitvalue);
```

- **ASCII字符串与网络字节序的二进制值之间的转换**
- 用于IP地址转换

```cpp
#include<arpa/inet.h>

/********************************************************************
 * 第一组：只支持IPv4；
 *        strptr：指向C字符串，表示点分十进制的地址
 *        addrptr/inaddr：网络字节序二进制值
 *        inet_addr函数：如今已废弃，新代码应该使用inet_aton（该函数出错时，
 *                      返回INADDR_NONE，32位均为1，因此255.255.255.255
 *                      不能由该函数处理）
 *        inet_ntoa函数：参数传入的是结构而不是结构的指针；
 *                      返回值所指字符串在静态区，故函数不可重入
 ********************************************************************/
int inet_aton(const char *strptr,struct in_addr *addrptr);//字符串有效返回1，否则0
int_addr_t inet_addr(const char *strptr);
char* inet_ntoa(struct in_addr inaddr);

 /**************************************************************
 * 第二组：既支持IPv4也支持IPv6；
 *        两个函数的family参数既可以是AF_INET，也可以是AF_INET6。如果
 *        以不被支持的地址族作为family参数，将返回一个错误，并将errno置
 *        为EAFNOSUPPORT；
 *        strptr：指向C字符串，表达式格式
 *        addrptr：网络字节序二进制值，数值格式
 *        len：strptr指向的内存区的大小，防止溢出
 ***************************************************************/
int inet_pton(int family,const char *strptr,void *addrptr);//成功返回1，字符串无效返回0，出错-1
const char* inet_ntop(int family,const void *addrptr,char *strptr,size_t len);
```

- **主机名和IP地址的转换**
```cpp
#include <netdb.h>
struct hostent * gethostbyname(const char * hostname);   //返回：若成功则为非空指针，若出错则为NULL且设置h_errno
int gethostbyname_r(const char *name,
               struct hostent *ret, char *buf, size_t buflen,
               struct hostent **result, int *h_errnop);
struct hostent{
    char *h_name;  //official name
    char **h_aliases;  //alias list
    int  h_addrtype;  //host address type
    int  h_length;  //address lenght
    char **h_addr_list;  //address list
}
```
- hostent结构
  - h_name：官方域名（Official domain name）。官方域名代表某一主页，但实际上一些著名公司的域名并未用官方域名注册。
  - h_aliases：别名，可以通过多个域名访问同一主机。同一 IP 地址可以绑定多个域名，因此除了当前域名还可以指定其他域名。
  - h_addrtype：gethostbyname() 不仅支持 IPv4，还支持 IPv6，可以通过此成员获取IP地址的地址族（地址类型）信息，IPv4 对应 AF_INET，IPv6 对应 AF_INET6。
  - h_length：保存IP地址长度。IPv4 的长度为 4 个字节，IPv6 的长度为 16 个字节。
  - h_addr_list：这是最重要的成员。通过该成员以整数形式保存域名对应的 IP 地址。对于用户较多的服务器，可能会分配多个 IP 地址给同一域名，利用多个服务器进行均衡负载。


## IPv4对应的结构

```cpp
struct sockaddr_in {
    sa_family_t    sin_family; /* address family: AF_INET */
    in_port_t      sin_port;   /* port in network byte order */
    struct in_addr sin_addr;   /* internet address */
};

struct in_addr {
    uint32_t       s_addr;     /* address in network byte order */
};

#define SERVER_ADDRESS "127.0.0.1"
#define SERVER_PORT     3000
struct sockaddr_in serveraddr;
serveraddr.sin_family = AF_INET;
serveraddr.sin_addr.s_addr = inet_addr(SERVER_ADDRESS);
serveraddr.sin_port = htons(SERVER_PORT);
```

## IPv6对应的结构
```cpp
struct sockaddr_in6
{
       sa_family_t sin6_family;   /* 地址协议簇： AF_INET6 */
       u_int16_t sin6_port;         /* 端口号， 要用网络字节序表示 */
       u_int32_t sin6_flowinfo     /* 流信息， 应设置为0 */
       struct in6_addr sin6_addr; /* ipv6 地址结构体， 见下面 */
       u_int32_t sin6_socpe_id;   /* scope ID， 尚处于实验阶段 */
}；

struct in6_addr
{
       unsigned char sa_addr[16]; /* ipv6地址， 要使用网络字节序表示 */
};
```

# 2.相关函数

## socket

- 用于创建一个socket描述符（socket descriptor），它唯一标识一个socket
  
```cpp        
#include <sys/socket.h>
int socket(int family, int type, int protocol);
```
- family，指定协议族，协议族决定了socket的地址类型，在通信中必须采用对应的地址
  - AF_INET，IPV4
  - AF_INET6,IPv6
  - AF_LOCAL
  - AF_ROUTE
- type指明套接字的类型
  - SOCK_STREAM, 流套接字,用于TCP
  - SOCK_DGRAM, 数据报套接字，用于UDP
  - SOCK_RAW, 原始套接字
  - SOCK_PACKET、SOCK_SEQPACKET
  - 可以加参数SOCK_NONBLOCK,SOCK_CLOEXEC.非阻塞
- protocol协议类型常值。设为0的话表示选择所给定family和type组合的系统默认值
  - IPPROTO_TCP， 用于TCP
  - IPPTOTO_UDP, 用于UDP
  - IPPROTO_SCTP、IPPROTO_TIPC
- CLOEXEC， close-on-exec
  - 这个文件描述符在fork子进程后执行exec时就关闭
  - 子进程以写时复制（COW，Copy-On-Write）方式获得父进程的数据空间、堆和栈副本，这其中也包括文件描述符。
  - 刚刚fork成功时，父子进程中相同的文件描述符指向系统文件表中的同一项（这也意味着他们共享同一文件偏移量）。
  - 接着，一般我们会调用exec执行另一个程序，此时会用全新的程序替换子进程的正文，数据，堆和栈等。此时保存文件描述符的变量当然也不存在了，我们就无法关闭无用的文件描述符了。
  - 所以通常我们会fork子进程后在子进程中直接执行close关掉无用的文件描述符，然后再执行exec
  - 但是在复杂系统中，有时我们fork子进程时已经不知道打开了多少个文件描述符（包括socket句柄等），这此时进行逐一清理确实有很大难度

## bind
- 对套接字进行端口和地址的绑定
- 通常服务器在启动的时候都会绑定一个众所周知的地址（如ip地址+端口号），用于提供服务，客户就可以通过它来接连服务器
- 而客户端就不用指定，有系统自动分配一个端口号和自身的ip地址组合。在connect()时由系统随机生成一个。

```cpp
#include <sys/socket.h> 
int bind(int sockfd, const struct sockaddr *myaddr, socklen_t addrlen);
```
- 返回： 若成功则为0，若出错则为-1
- 第二个参数是一个指向特定的地址结构的指针，第三个参数是该地址结构的长度。
  - 地址结构根据创建socket时的地址协议族的不同而不同
- 调用bind函数可以指定一个端口号，或指定一个IP地址，也可以两者都指定，还可以不指定.
- 服务器在启动时捆绑众所周知端口，客户一般让内核选择临时端口
- Tcp客户端通常不把IP地址捆绑到它的套接字上，当连接套接字时，内核将根据所用外出网络接口来选择源IP地址
- 如果TCP服务器没有把IP地址捆绑到它的套接字上，内核就把客户发送的SYN的目的地址作为服务器的源IP地址

|IP地址|端口|结果|
|----|----|----|
|通配地址|0|内核选择IP地址和端口|
|通配地址|非0|内核选择IP地址，进程指定端口|
|本地IP地址|0|进程指定IP地址，内核选择端口|
|本地IP地址|非0|进程指定IP地址和端口|

## listen

- 作为一个服务器，在调用socket()、bind()之后就会调用listen()来监听这个socket，如果客户端这时调用connect()发出连接请求，服务器端就会接收到这个请求。
- socket()函数创建的socket默认是一个主动类型的，listen函数将socket变为被动类型的，等待客户的连接请求

```cpp
#include <sys/socket.h>
int listen(int sockfd, int backlog);
```
- 第一个参数即为要监听的socket描述字，第二个参数为相应socket可以排队的最大连接个数
- 内核为任一给定的监听套接字维护两个队列，两个队列之和不超过backlog：
  - **未完成连接队列**
    - 每个这样的SYN分节对应其中一项，已由某个客户发出并到达服务器，而服务器正在等待完成相应的TCP三路握手过程。这些套接字处于SYN_RCVD状态。
    - 在未完成连接队列中连接，若在RTT时间内还未完成三次握手，则超时并从该队列中删除
  - **已完成连接队列**
    - 每个已完成TCP三路握手过程的客户对应其中一项。这些套接字处于ESTABLISHED状态
    - 当进程调用accept时，如果已连接队列不为空，那么队头项将返回给进程，否则进程将投入睡眠，直到TCP在该队列中放入一项才唤醒它
- **SYN到达时，如果队列已满，TCP忽略该SYN分节**
  - 这么做是因为这种情况是暂时的，这种处理方式希望通过重传机制，在客户端下一次重传时，队列中已经存在可用空间。如果服务器立即响应RST，客户的connect调用就会立即返回一个错误，强制应用程序处理这种情况。另外，客户也无法区分RST是意味着“该端口没有服务器在监听”还是意味着“队列已满”

## accept

- 用于从已完成连接队列队头返回下一个已完成连接，如果已完成连接队列为空，那么进程被投入睡眠
```cpp
int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);
int accept4(int sockfd, struct sockaddr *addr,socklen_t *addrlen, int flags);
// 可以在新开的文件描述符中设置SOCK_NONBLOCK | SOCK_CLOEXEC
```
- 第一个参数为服务器的socket描述字(监听套接字)
- 第二个参数为指向struct sockaddr *的指针，用于返回客户端的协议地址，第三个参数为协议地址的长度。如果不关心，可以都设置为空
- 如果accpet成功，那么其返回值是由内核自动生成的一个全新的描述字（已连接套接字），代表与返回客户的TCP连接。
- 非阻塞的套接字在没有等待的连接时会返回-1，并且错误码设置为EAGAIN
- 监听套接字和已连接套接字
  - 监听套接字：由socket创建，随后用作bind和listen的第一个参数的描述符
  - 已连接套接字：accept的返回值
  - 一个服务器通常仅仅创建一个监听套接字，它在该服务器的生命期内一直存在
  - 内核为每个由服务器进程接受的客户连接创建一个已连接套接字，当服务器完成对某个给定客户的服务时，相应的已连接套接字就被关闭
- 获取新连接的方式
  - 每次accept一个socket
  - 每次循环accept，直到没有新连接到达
  - 每次尝试accept N个新连接，N的值一般是10
  - 后面两种方法适合短连接，这里采用的是第一种方法
- EMFILE,文件描述符耗尽处理方法
  - 当accept返回EMFILE时，意味着本进程的文件描述符已经达到上限，无法为新连接创建套接字文件描述符。但是，既然没有文件描述符来表示这个连接，也就无法close它，这个新连接会处于一直等待处理的状态。每次epoll_wait都会立刻返回。会使程序陷入busy loop。
  - 有7种解决做法，本程序选用的是第6种
    - 调高进程的文件描述符数目。治标不治本
    - 死等
    - 退出程序
    - 关闭监听套接字
    - 用ET取代LT,不会陷入无限循环的状态
    - 提前准备一个空闲的文件描述符，遇到EMFILE时，先关闭这个空闲文件，获得一个文件描述符的名额，再accept拿到新连接的描述符，随后立刻close它，最后再重新打开一个空闲文件。
    - 自己设置一个稍微低点的文件描述符数量限制，超过这个限制就主动关闭新连接。

## connect, 连接套接字
```cpp
#include<sys/socket.h>
int connect(int sockfd, const struct sockaddr *servaddr, socklen_t addrlen);
```
- 返回：若成功则为0，若出错则为-1 
- 第二个参数，第三个参数分别是一个指向要连接的套接字地址结构的指针和该结构的大小。套接字地址结构必须包含有服务器的IP地址和端口号。
- 客户在调用函数connect前不必非得调用bind函数，因为需要的话，内核会确定源IP地址，并选择一个临时端口作为源端口。
- 如果是TCP套接字，调用connect函数将激发TCP的三路握手过程，而且仅在成功或错误时才退出，其中出错返回可能有以下几种情况：
    - 若TCP客户没有收到SYN分节的响应，则返回ETIMEDOUT错误。
    - 若对客户的SYN响应是RST（表示复位），则表明该服务器主机在我们指定的端口上没有进程在等待与之连接（例如服务器进程也许没有运行），返回ECONNREFUSED错误。产生RST的三个条件是：目的地为某端口的SYN到达，然而该端口上没有正在监听的服务器；TCP想取消一个已有连接；TCP接收到一个根本不存在的连接上的分节。
    - 客户发出的SYN在中间的某个路由器上引发目的地不可达。客户主机内核保存该消息，并增大间隔时间重发SYN。若在某个规定的时间后仍未收到响应，则把保存的消息（ICMP错误）作为EHOSTTUNREACH或ENETUNREACH错误返回给进程。
- 若connect失败则套接字不再可用，必须关闭。需要close当前的套接字描述符并重新调用socket。
- 非阻塞套接字调用connect，如果返回0，表示连接已经完成，如果返回-1，那么期望收到的错误是EINPROGRESS，表示连接建立已经启动，但是尚未完成。
- 对于非阻塞套接字调用connect不会立即完成，通常返回错误-1，错误码是EINPROGRESS，我们应该调用select或者poll等待套接字可写，然后使用getsockopt获取错误值，如果等于0就是连接成功。


## 数据传输函数
- 有好几组
  - read()/write()
  - recv()/send()
  - readv()/writev()
  - recvmsg()/sendmsg()
  - recvfrom()/sendto()，一般用于UDP数据传输
```cpp
#include <unistd.h>
ssize_t read(int fd, void *buf, size_t count);
ssize_t write(int fd, const void *buf, size_t count);
#include <sys/types.h>
#include <sys/socket.h>
ssize_t send(int sockfd, const void *buf, size_t len, int flags);
ssize_t recv(int sockfd, void *buf, size_t len, int flags);

ssize_t sendto(int sockfd, const void *buf, size_t len, int flags,
                      const struct sockaddr *dest_addr, socklen_t addrlen);
ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags,
                        struct sockaddr *src_addr, socklen_t *addrlen);

ssize_t sendmsg(int sockfd, const struct msghdr *msg, int flags);
ssize_t recvmsg(int sockfd, struct msghdr *msg, int flags);
```

## 关闭套接字

```cpp
#include <unistd.h>
int close(int fd);
#include<sys/socket.h>
int shutdown(int fd, int howto);
//成功为0，出错返回-1
```

- close一个TCP套接字的默认行为是把套接字标记为关闭，立即返回调用进程，然后TCP将尝试发送已排队等待发送到对端的任何数据，发送完毕后是正常的TCP连接终止序列
- howto的可选值
    - `SHUT_RD`：**关闭连接的读这一半**，套接字接收缓冲区中的现有数据都被丢弃。进程不能再对这样的套接字调用任何读函数（对一个TCP套接字这样调用shutdown函数后，由该套接字接收的来自对端的任何数据都被确认，然后悄然丢弃）
    - `SHUT_WR`：**关闭连接的写这一半**（**对于TCP，称为半关闭**），套接字发送缓冲区中的数据将被发送掉，后跟TCP的正常连接终止序列。进程不能再对这样的套接字调用任何写函数
    - `SHUT_RDWR`：**连接的读半部和写半部都关闭**。等价于调用2次shutdown，分别指定SHUT_RD与SHUT_WR
- shutdown与close的区别
  - 关闭套接字的时机不同
    - close把描述符的引用计数减1，仅在该计数变为0时才关闭套接字
    - shutdown不管引用计数就能激发TCP的正常连接终止序列
  - 全关闭与半关闭
    - close终止读和写两个方向的数据传送
    - shutdown可以只关闭一个方向的数据传送（具体见上面的howto参数）
  

## 获取源IP、源端口号、目的IP、目的端口号的方法

- TCP服务端
  - 通过accept获得源IP地址和源端口号
  - getsockname获得目的IP地址和端口号
- UDP服务器
  - 通过recvfrom获得源IP地址和源端口号
  - getsockname获得目的IP地址和端口号

- 适用的情况
  - 没有调用bind的客户端不知道自己的源IP/端口
  - 调用了bind也会因为选择了通配IP地址或端口号0，而不知道确切的IP/端口
```cpp
#include<sys/socket.h>
int getsockname(int sockfd,struct sockaddr *addr, socklen_t *addrlen);
int getpeername(int sockfd,struct sockaddr *addr, socklen_t *addrlen);
```
- getsockname用于返回与某个套接字关联的本地协议地址
  - 没调用bind的TCP客户上，connect返回后，获取由内核赋予该连接的本地IP地址和端口号
  - 以端口号0调用bind后，获取由内核赋予的本地端口号
  - 获取某个套接字的地址族
  - 绑定通配IP的服务器上，accept返回后，获取由内核赋予该连接的本地IP地址，此时sockfd不是监听套接字描述符
- getpeername用于返回与某个套接字关联的外地协议地址
    - 当一个服务器是由调用过accept的某个进程通过调用exec执行程序时，它能获取客户身份的唯一途径便是调用getpeername。例如inetd fork并exec某个TCP服务器程序：


# 3.TCP通信

- 服务端：socket -> bind -> listen -> accept -> recv/send -> close。
- 客户端：socket -> connect -> send/recv -> close。

TCP连接的基本流程：
服务器：首先调用socket()创建一个套接字用来通讯，其次调用bind()进行绑定这个文件描述符，并调用listen()用来监听端口是否有客户端请求来，如果有，就调用accept()进行连接，否则就继续阻塞式等待直到有客户端连接上来。连接建立后就可以进行通信了。

客户端：调用socket()分配一个用来通讯的端口，接着就调用connect()发出SYN请求并处于阻塞等待服务器应答状态，服务器应答一个SYN-ACK分段，客户端收到后从connect()返回，同时应答一个ACK分段，服务器收到后从accept()返回，连接建立成功。客户端一般不调用bind()来绑定一个端口号，并不是不允许bind()，服务器也不是必须要bind()。

为什么不建议客户端进行bind()？
答：当客户端没有自己进行bind时，系统随机分配给客户端一个端口号，并且在分配的时候，操作系统会做到不与现有的端口号发生冲突。但如果自己进行bind，客户端程序就很容易出现问题，假设在一个PC机上开启多个客户端进程，如果是用户自己绑定了端口号，必然会造成端口冲突，影响通信。



# 4.UDP通信

- 客户端要发起一次请求，仅仅需要两个步骤（socket和sendto)
- 而服务器端也仅仅需要三个步骤即可接收到来自客户端的消息（socket、bind、recvfrom）
- 服务端：socket -> bind -> recvfrom/sendto -> close。
- 客户端：socket -> sendto/recvfrom -> close
## 缓冲区

UDP 只有一个 socket 接收缓冲区，没有 socket 发送缓冲区，即只要有数据就发，不管对方是否可以正确接收。而在对方的 socket 接收缓冲区满了之后，新来的数据报无法进入到 socket 接受缓冲区，此数据报就会被丢弃，因此 UDP 不能保证数据能够到达目的地，此外，UDP 也没有流量控制和重传机制，故UDP的数据传输是不可靠的。



## 数据传输

- 一般使用sendto和revfrom传输数据
```cpp
ssize_t sendto(int sockfd, const void *buf, size_t len, int flags,
                      const struct sockaddr *dest_addr, socklen_t addrlen);
ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags,
                        struct sockaddr *src_addr, socklen_t *addrlen);
```

- 参数说明
  - s： socket描述符。
  - buf： UDP数据报缓存地址。
  - len： UDP数据报长度。
  - flags： 该参数一般为0。
  - to： sendto()函数参数，struct sockaddr_in类型，指明UDP数据发往哪里报。
  - tolen： 对方地址长度，一般为：sizeof(struct sockaddr_in)。
  - from： recvfrom()函数参数，struct sockaddr_in类型，指明从哪里接收UDP数据报。
  - fromlen: 对方地址长度，一般为：sizeof(struct sockaddr_in)。
- 函数返回值
  - 对于sendto()函数，成功则返回实际传送出去的字符数，失败返回-1，错误原因存于errno 中。
  - 对于recvfrom()函数，成功则返回接收到的字符数，失败则返回-1，错误原因存于errno中。
- 必须给sendto调用指定**服务器的IP地址和端口号**
- recvfrom的from参数可以是一个空指针，此时addrlen也必须是一个空指针，表示并不关心数据发送者的协议地址
- 写一个长度为0的数据报是可行的。
  - 在UDP情况下，这会形成一个只包含一个IP首部（对于IPv4通常为20字节，对于IPv6通常为40字节）和一个8字节UDP首部而没有数据的IP数据报。
  - 这也意味着对于数据报协议，recvfrom返回0值是可接受的：它并不像TCP套接字上read返回0值那样表示对端已关闭连接。既然UDP是无连接的，因此也就没有诸如关闭一个UDP连接之类的事情

## UDP调用connect

- 并不触发三次握手
  - UDP中调用 connect 只是把对端的 IP 和 端口号记录下来
  - 检查是否存在立即可知的错误
- 已连接的UDP套接字与默认的未连接UDP套接字的区别：
  - 限制了一个已连接套接字能且仅能与一个对端交换数据
    - 不能再给输出操作指定目的IP地址和端口号，即不使用sendto而改用write或send
    - 不必使用recvfrom以获悉数据报的发送者，而改用read、recv或recvmsg
    - 发源地不是该套接字早先connect到的协议地址的数据报不会投递到该套接字
  - 接收异步错误
    - 由已连接UDP套接字引发的异步错误会返回给它们所在的进程。未连接时不接收任何异步错误
- 可以直接再次调用connect
  - 指定新的IP地址和端口号
  - 断开套接字：调用connect时，把套接字地址结构的地址族成员设置为AF_UNSPEC
  - TCP要再次调用connect必须先close套接字再重新调用socket创建套接字描述符
- 连接与不连接的性能对比
  - 当应用进程在一个未连接的UDP套接字上调用sendto时，源自Berkeley的内核暂时连接该套接字，发送数据报，然后断开该连接。因此，当应用进程要给同一目的地址发送多个数据报时，使用连接套接字可以获得更高的效率
  - 和普通的 UDP 相比，调用 connect 的 UDP 会提升效率，并且在高并发服务中会增加系统稳定性。



# 5.IO模型

- 一个输入操作通常包括两个阶段
  - 等待数据准备好
  - 从内核向进程复制数据
- 对于一个套接字上的输入操作
  - 第一步通常涉及等待数据从网络中到达。当所等待数据到达时，它被复制到内核中的某个缓冲区。
  - 第二步就是把数据从内核缓冲区复制到应用进程缓冲区。











## 阻塞和非阻塞I/O区别

- 阻塞表示一旦调用I/O函数必须等整个I/O完成才返回。
  - 例如当服务器调用了read函数之后，如果不是立即接收到数据，服务器进程会被阻塞，之后一直在等待用户数据到达，用户数据到达后首先会写进内核缓冲区，之后内核缓冲区数据复制到用户进程（服务器进程）缓冲区。完成了上述所有的工作后，才会把执行权限返回给用户（从内核态 -> 用户态）。
- 阻塞式I/O的效率实在太低，如果用户输入数据迟迟不到的话，整个服务器就会一直被阻塞（单进程/线程）。
- 解决方法
  - 为了不影响服务器接收其他进程的连接，我们可以考虑多进程模型，这样当服务器建立连接后为连接的用户创建新线程，新线程即使是使用阻塞式I/O也仅仅是这一个线程被阻塞，不会影响服务器等待接收新的连接。
  - 多线程模型下，主线程等待用户请求，用户有请求到达时创建新线程。新线程负责具体的工作，即使是因为调用了read函数被阻塞也不会影响服务器。我们还可以进一步优化创建连接池和线程池以减小频繁调用I/O接口的开销。但新问题随之产生，每个新线程或者进程（加入使用对进程模型）都会占用大量系统资源，除此之外过多的线程和进程在调度方面开销也会大很对，所以这种模型并不适合大并发量。
- 阻塞和非阻塞最大的区别在于调用I/O系统调用后，是等整个I/O过程完成再把操作权限返回给用户还是会立即返回。
- 可以使用以下语句将句柄fd设置为非阻塞I/O：fcntl(fd, F_SETFL, O_NONBLOCK);
- 非阻塞I/O在调用后会立即返回，用户进程对返回的返回值判断以区分是否完成了I/O。如果返回大于0表示完成了数据读取，返回值即读取的字节数；返回0表示连接已经正常断开；返回-1表示错误，并设置errno为EAGAIN。
- 非阻塞I/O虽然不再会完全阻塞用户进程，但实际上由于用户进程需要不停地询问kernel是否准备完数据，所以整体效率依旧非常低，不适合做并发。

## 同步和异步区别

- 同步I/O指产生I/O操作的进程和处理I/O操作的进程是同一个。
- 异步I/O中I/O操作由操作系统完成，并不由产生I/O的用户进程执行。用户进程发起I/O请求后立即返回，直到内核发送一个信号，告知进程I/O已完成，在整个过程中，都没有进程被阻塞。

## 常见的5种

- 阻塞式I/O
  - 应用进程被阻塞，直到数据从内核缓冲区复制到应用进程缓冲区中才返回
  - 在阻塞的过程中，其它应用进程还可以执行，因此阻塞不意味着整个操作系统都被阻塞。因为其它应用进程还可以执行，所以不消耗 CPU 时间，这种模型的 CPU 利用率会比较高。
- 非阻塞式I/O
  - 应用进程执行系统调用之后，内核返回一个错误码。应用进程可以继续执行，但是需要不断的执行系统调用来获知I/O是否完成，这种方式称为轮询（polling）。
  - 由于CPU要处理更多的系统调用，因此这种模型的 CPU 利用率比较低。
- I/O复用
  - 使用 select，poll，epoll 等待数据，并且可以等待多个套接字中的任何一个变为可读。这一过程会被阻塞，当某一个套接字可读时返回
  - 让单个进程具有处理多个 I/O 事件的能力
  - 事件驱动 I/O
- 信号驱动 I/O
  - 应用进程使用 sigaction 系统调用，内核立即返回，应用进程可以继续执行，也就是说等待数据阶段应用进程是非阻塞的。
  - 内核在数据到达时向应用进程发送 SIGIO 信号，应用进程收到之后在信号处理程序中调用 recvfrom 将数据从内核复制到应用进程中。
- 异步I/O
  - 应用进程执行 aio_read 系统调用会立即返回，应用进程可以继续执行，不会被阻塞，内核会在所有操作完成之后向应用进程发送信号
  - 异步 I/O 与信号驱动 I/O 的区别
    - 异步 I/O 的信号是通知应用进程 I/O 完成
    - 信号驱动 I/O 的信号是通知应用进程可以开始 I/O


# 6.IO复用

- **I/O复用是一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。**
- 本质上这些I/O复用技术都是同步I/O，在读写事件就绪后需要进程自己负责进行读写，即读写过程是进程阻塞的
- IO多路复用和非阻塞IO配合使用
  - 单纯的非阻塞IO需要应用程序轮询来检查IO操作是否完成，太浪费CPU
  - 如果IO多路复用和阻塞IO一起使用，因为阻塞IO会阻塞当前线程，使线程无法处理其他socket上的IO事件

## select

```cpp
#include <sys/select.h>
#include <sys/time.h>
int select(int maxfdp,fd_set *readfds,fd_set *writefds,fd_set *errorfds,struct timeval *timeout);
struct timeval{
    long tv_sec;    /* 秒 */
    long tv_usec;   /* 微妙 */
void FD_ZERO(fd_set *fdset);            //清除fdset的所有位
void FD_SET(int fd,fd_set *fdset);      //打开fdset中的fd位
void FD_CLR(int fd,fd_set *fdset);      //清除fdset中的fd位
int FD_ISSET(int fd,fd_set *fdset);     //检查fdset中的fd位是否置位
};
```
- 5个参数
  - `maxfdp`：指定待测试的描述符个数，值为待测试的最大描述符加1（参数名的由来）
  - `readset`：读描述符集
  - `writeset`：写描述符集
  - `exceptset`：异常描述符集。
  - `timeout`：(告知)内核等待任意描述符就绪的超时时间
    - 永远等待下去：设为空指针
    - 等待一段固定时间
    - 立即返回(轮询)：timeval结构中的时间设为0
- 返回值
  - 0：超时
  - -1：出错
  - 正值:正常
- fd_set,描述符集
  - 是一个整数数组，其中每个整数中的一位对应一个描述符
  - FD_SET是有限的，即内核中有个参数__FD_SETSIZE定义了每个FD_SET的句柄个数
  - 如果对三个描述符集中的某个不感兴趣，可以设为空指针，如果都设为空指针，会得到一个比Unix的sleep函数更精确的定时器（sleep以秒为最小单位）
  - select调用返回时，结果将指示哪些描述符已就绪。函数返回后，使用FD_ISSET宏来测试fd_set数据类型中的描述符
  - **描述符集内任何与未就绪描述符对应的位返回时均清为0。因此，每次重新调用select函数时，都得再次把所有描述符集内所关心的位设置为1**
- Select工作流程
  - 用FD_ZERO宏来初始化我们感兴趣的fd_set。
  - 用FD_SET宏来将套接字句柄分配给相应的fd_set。
  - 调用select函数
  - select函数返回后，用FD_ISSET对套接字句柄进行检查。
- 描述符就绪条件
  - 套接字可读
    - 套接字接收缓冲区中的数据(字节数)大于等于套接字接收缓冲区低水位标记的当前大小
    - 连接的读半部关闭，对这样的套接字读不会阻塞并返回0(EOF)
    - 监听套接字已完成的连接数不为0。对其accept通常不阻塞
    - 套接字上有一个错误，对其读不会阻塞并返回-1，同时把errno设为确切错误条件
  - 套接字可写
    - 套接字发送缓冲区中的可用空间(字节数)大于等于套接字发送缓冲区低水位标记的当前大小，并且或者该套接字已连接，或者该套接字不需要连接(如UDP套接字)
    - 连接的写半部关闭，对这样的套接字写将产生SIGPIPE信号
    - 使用非阻塞式connect的套接字已建立连接，或者connect以失败告终
    - 套接字上有一个错误，对其写将不会阻塞并返回-1，同时把errno设为确切错误条件
  - 套接字有异常条件待处理
- select的优缺点
  - 优点
    - 跨平台支持好，目前几乎在所有平台上支持
  - 缺点
    - 最大的缺点是，进程打开的fd有限（由`FD_SETSIZE`和内核决定，一般为1024），这对于连接数量比较大的服务器来说根本不能满足（可以选择多进程的解决方案，虽然Linux上创建进程的代价比较小，但也不可忽视，加上进程间数据同步远比不上线程间同步的效率，所以也不是一种完美的方案）
    - 函数返回后需要轮询描述符集，寻找就绪描述符，效率不高
    - 用户态和内核态传递描述符结构时copy开销大

**增大描述符集大小的唯一方法是：先增大`FD_SETSIZE`的值，再重新编译内核，不重新编译内核而改变其值是不够的**

## poll

- poll的机制与select类似，与select在本质上没有多大差别，管理多个描述符也是进行轮询，根据描述符的状态进行处理，但是poll没有最大文件描述符数量的限制。
- poll和select同样存在一个缺点就是，包含大量文件描述符的数组被整体复制于用户态和内核的地址空间之间，而不论这些文件描述符是否就绪，它的开销随着文件描述符数量的增加而线性增大

```cpp
#include <poll.h>
int poll (struct pollfd * fds, unsigned int nfds, int timeout);
//返回：若有就绪描述符则为其数目，若超时则为0，若出错则为-1
//返回后需要轮询所有描述符来获取已经就绪的描述符
struct pollfd {
int fd;              //监视的描述符
short events;        //该描述符上监视的事件
short revents;       //该描述符上发生的事件
};
```
- 参数
  - `fds`
    - 关心的描述符和事件
    - 如果不再关心某个特定描述符，可以把与之对应的pollfd结构的fd成员设置成一个负值。poll函数将忽略这样的pollfd结构的events成员，返回时将其revents成员的值置为0
  - `nfds`：pollfd数组的元素个数（即监视的描述符总数）
  - `timeout`：(告知)内核等待任意描述符就绪的超时时间，超时函数返回0
    - `INFTIM`（一个负值）：永远等待下去
    - `>0`：等待一段固定时间
    - `0`：立即返回(轮询)
- 事件
  - POLLIN 普通或优先级带数据可读。
  - POLLRDNORM 普通数据可读。
  - POLLRDBAND　优先级带数据可读。
  - POLLPRI　高优先级数据可读。
  - POLLOUT　普通数据可写。
  - POLLWRNORM　普通数据可写不会导致阻塞。
  - POLLWRBAND　优先级带数据可写。
  - POLLMSGSIGPOLL 消息可用。
- 优缺点
  - 优点
    - **没有最大监视描述符数量的限制**：分配一个pollfd结构的数组并把该数组中元素的数目通知内核成了调用者的责任。内核不再需要知道类似fd_set的固定大小的数据类型
  - 缺点
    - 和select一样，调用返回后需要**轮询所有描述符来获取已经就绪的描述符**
    - **用户态和内核态传递描述符结构时copy开销大*


## select和poll对比
- select 和 poll 的功能基本相同，不过在一些实现细节上有所不同。
  - select 会修改描述符，而 poll 不会；
  - select 的描述符类型使用数组实现，FD_SETSIZE 大小默认为 1024，因此默认只能监听 1024 个描述符。如果要监听更多描述符的话，需要修改 FD_SETSIZE 之后重新编译；而 poll 的描述符类型使用链表实现，没有描述符数量的限制；
  - poll 提供了更多的事件类型，并且对描述符的重复利用上比 select 高。
  - 如果一个线程对某个描述符调用了 select 或者 poll，另一个线程关闭了该描述符，会导致调用结果不确定。
- 速度
  - select 和 poll 速度都比较慢。
  - select 和 poll 每次调用都需要将全部描述符从应用进程缓冲区复制到内核缓冲区。
  - select 和 poll 的返回结果中没有声明哪些描述符已经准备好，所以如果返回值大于 0 时，应用进程都需要使用轮询的方式来找到 I/O 完成的描述符。
- 可移植性
  - 几乎所有的系统都支持 select，但是只有比较新的系统支持 poll。


## epoll

### 1.创建epoll文件描述符

- 创建一个epoll的文件描述符，size用来告诉内核这个监听的数目一共有多大。这个参数不同于select()中的第一个参数，select的是最大监听的fd+1的值
  - 从Linux 2.6.8开始，size参数被忽略，但必须大于零
  - 现在内核动态的分配描述事件需要的空间
- 成功返回一个非0 的未使用过的最小的文件描述符
- 失败返回-1 errno被设置
- 会占用文件描述符，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。
- epoll_create1中的flag
  - 如果这个参数是0，这个函数等价于epoll_create（0）
  - EPOLL_CLOEXEC：这是这个参数唯一的有效值，如果这个参数设置为这个。那么当进程替换映像的时候会关闭这个文件描述符，这样新的映像中就无法对这个文件描述符操作，适用于多进程编程+映像替换的环境里
```cpp
int epoll_create(int size);
int epoll_create1(int flags);

```
### 2. 注册事件
```cpp
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
// 返回0成功，返回-1失败
struct epoll_event {
    __uint32_t events; //epoll事件
    epoll_data_t data; //用户数据
};
typedef union epoll_data {
    void *ptr;
    int fd;
    __uint32_t u32;
    __uint64_t u64;
} epoll_data_t;
```
  - 第一个参数是epoll_create()的返回值,也就是一个epoll文件描述符
  - 第二个参数表示动作，用三个宏来表示：
    - EPOLL_CTL_ADD：注册新的fd到epfd中；
    - EPOLL_CTL_MOD：修改已经注册的fd的监听事件；
    - EPOLL_CTL_DEL：从epfd中删除一个fd；
  - 第三个参数是需要监听的fd
  - 第四个参数是告诉内核需要监听什么事
    - events`成员可以是下列宏的集合：
      - EPOLLIN：对应的描述符可读
      - EPOLLOUT：对应的描述符可写
      - EPOLLPRI：对应的描述符有紧急数据可读（带外数据）
      - EPOLLERR：对应的描述符发生错误
      - EPOLLHUP**：对应的描述符被挂断
      - **EPOLLET**：将epoll设为**边缘触发模式**（默认为**水平(LT)触发模式**）
      - **EPOLLONESHOT**：只监听一次事件，监听完后，如果需要再次监听，需再次将描述符加入到epoll队列
  - 已注册的描述符在内核中会被维护在一棵红黑树上，通过回调函数内核会将 I/O 准备好的描述符加入到一个链表中管理
 
### 3. 等待事件
```cpp
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```
- 返回需要处理的事件数目
- 第一个参数是epoll_create()的返回值
- 参数events用来从内核得到事件的集合，所有就绪的事件从内核事件表中复制到它的第二个参数events指向的数组中
- maxevents：返回事件的最大数量，不能大于创建epoll_create()时的size
- 参数timeout是超时时间（毫秒）
  - 0会立即返回
  - -1将永久阻塞

### EPOLLONESHOT事件
- 使用场合
  - 一个线程在读取完某个socket上的数据后开始处理这些数据，而数据的处理过程中该socket又有新数据可读，此时另外一个线程被唤醒来读取这些新的数据。
  - 于是，就出现了两个线程同时操作一个socket的局面。可以使用epoll的EPOLLONESHOT事件实现一个socket连接在任一时刻都被一个线程处理。
- 作用
  - 对于注册了EPOLLONESHOT事件的文件描述符，操作系统最多出发其上注册的一个可读，可写或异常事件，且只能触发一次。
- 使用
  - 注册了EPOLLONESHOT事件的socket一旦被某个线程处理完毕，该线程就应该立即重置这个socket上的EPOLLONESHOT事件，以确保这个socket下一次可读时，其EPOLLIN事件能被触发，进而让其他工作线程有机会继续处理这个sockt。

### LT和ET

- **水平触发(LT)模式**（**默认**）：
  - 应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次向应用程序通知此事件
  - 同时支持block和no-block socket
- **边缘触发(ET)模式**（高速工作方式）：
  - 应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不再向应用程序通知此事件
  - 只支持no-block socket（以免一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死）
- 阻塞不能用ET，非阻塞都可以
  - ET模式下每次write或read需要循环write或read直到返回EAGAIN错误
  - 因为ET模式只在socket描述符状态发生变化时才触发事件，如果不一次把socket内核缓冲区的数据读完，会导致socket内核缓冲区中即使还有一部分数据
  - 若ET模式下使用阻塞IO，则程序一定会阻塞在最后一次write或read操作，因此说ET模式下一定要使用非阻塞IO。

**ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高**

### epoll的优缺点：

- 优点
  - 监视的描述符数量不受限制**，所支持的`fd`上限是最大可以打开文件的数目（一般远大于2048，和系统内存关系较大，可以使用`cat /proc/sys/fs/file-max`查看）
  - I/O效率不会随着监视fd数量的增长而下降**：epoll不同于select和poll轮询的方式，而是通过每个fd定义的回调函数来实现的，只有就绪的fd才会执行回调函数
  - 用户态和内核态消息传递的开销小**
- 缺点
  - 如果没有大量的“idle连接”或“dead连接”，epoll的效率并不会比select/poll高很多
  - 当连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好
  - 当遇到大量的"idle连接"，epoll的效率会大大高于select/poll

#### 机制实现
当某一进程调用epoll_create方法时，Linux内核会创建一个eventpoll结构体，这个结构体中有两个成员与epoll的使用方式密切相关。eventpoll结构体如下所示：

```cpp
struct eventpoll{
    /*红黑树的根节点，这颗树中存储着所有添加到epoll中的需要监控的事件*/
    struct rb_root  rbr;
    /*双链表中则存放着将要通过epoll_wait返回给用户的满足条件的事件*/
    struct list_head rdlist;
};
```
- 每一个epoll对象都有一个独立的eventpoll结构体，用于存放通过epoll_ctl方法向epoll对象中添加进来的事件。这些事件都会挂载在红黑树中，如此，重复添加的事件就可以通过红黑树而高效的识别出来(红黑树的插入时间效率是lgn，其中n为树的高度)。
- 而所有添加到epoll中的事件都会与设备(网卡)驱动程序建立回调关系，也就是说，当相应的事件发生时会调用这个回调方法。这个回调方法在内核中叫ep_poll_callback,它会将发生的事件添加到rdlist双链表中。
- 当调用epoll_wait检查是否有事件发生时，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可。如果rdlist不为空，则把发生的事件复制到用户态，同时将事件数量返回给用户

## 三者对比

- select 的 timeout 参数精度为 1ns，而 poll 和 epoll 为 1ms，因此 select 更加适用于实时性要求比较高的场景，比如核反应堆的控制
- select 可移植性更好，几乎被所有主流平台所支持
- poll 没有最大描述符数量的限制，如果平台支持并且对实时性要求不高，应该使用 poll 而不是 select
- epoll适用于有大量的描述符需要同时轮询，并且这些连接最好是长连接
- 当连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好

## epoll和select的区别，epoll为什么高效

- 区别
  - 每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大；而epoll保证了每个fd在整个过程中只会拷贝一次。
  - 每次调用select都需要在内核遍历传递进来的所有fd；而epoll只需要轮询一次fd集合，同时查看就绪链表中有没有就绪的fd就可以了。
  - select支持的文件描述符数量太小了，默认是1024；而epoll没有这个限制，它所支持的fd上限是最大可以打开文件的数目，这个数字一般远大于2048。
- epoll为什么高效
  - select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。
  - select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把当前进程往设备等待队列中挂一次，而epoll只要一次拷贝，而且把当前进程往等待队列上挂也只挂一次，这也能节省不少的开销
  - poll每次返回整个文件描述符数组，用户代码需要遍历数组以找到哪些文件描述符上有IO事件
  - epoll返回的是活动fd的列表，需要遍历的数组通常会小很多
  - 在并发连接数较大而活动连接比例不高时，epoll比poll高效

# 套接字编程常见的异常行为

https://www.cnblogs.com/promise6522/archive/2012/03/03/2377935.html



- 假设A机器上的一个进程a正在和B机器上的进程b通信：某一时刻a正阻塞在socket的read调用上（或者在nonblock下轮询socket）
- 当b进程终止时，无论应用程序是否显式关闭了socket（OS会负责在进程结束时关闭所有的文件描述符，对于socket，则会发送一个FIN包到对面）。
- ”同步通知“：进程a对已经收到FIN的socket调用read，如果已经读完了receive buffer的剩余字节，则会返回EOF:0
- ”异步通知“：如果进程a正阻塞在read调用上（前面已经提到，此时receive buffer一定为空，因为read在receive buffer有内容时就会返回），则read调用立即返回EOF，进程a被唤醒。
- socket在收到FIN后，虽然调用read会返回EOF，但进程a依然可以其调用write，因为根据TCP协议，收到对方的FIN包只意味着对方不会再发送任何消息。 在一个双方正常关闭的流程中，收到FIN包的一端将剩余数据发送给对面（通过一次或多次write），然后关闭socket。




## SIGPIPE

- 对一个已关闭的套接字第一次调用write会返回RST
- 如果对收到RST的套接字再一次调用write，会返回SIGPIPE
- 收到SIGPIPE的默认处理动作是终止进程，知道你的进程为什么毫无征兆地死亡了吧：）


## 解决对端崩溃问题

- 不同于b进程退出（此时OS会负责为所有打开的socket发送FIN包），当B机器的OS崩溃（注意不同于人为关机，因为关机时所有进程的退出动作依然能够得到保证）/主机断电/网络不可达时，a进程根本不会收到FIN包作为连接终止的提示。
- 如果a进程阻塞在read上，那么结果只能是永远的等待。
- 如果a进程先write然后阻塞在read，由于收不到B机器TCP/IP栈的ack，TCP会持续重传12次（时间跨度大约为9分钟），然后在阻塞的read调用上返回错误：ETIMEDOUT/EHOSTUNREACH/ENETUNREACH
- 假如B机器恰好在某个时候恢复和A机器的通路，并收到a某个重传的pack，因为不能识别所以会返回一个RST，此时a进程上阻塞的read调用会返回错误ECONNREST

恩，socket对这些错误还是有一定的反馈能力的，前提是在对面不可达时你依然做了一次write调用，而不是轮询或是阻塞在read上，那么总是会在重传的周期内检测出错误。如果没有那次write调用，应用层永远不会收到连接错误的通知。

write的错误最终通过read来通知应用层，有点阴差阳错？

- 解决方法
  - KEEPALIVE
  - 进行应用层的心跳
    - 针对连接做timeout，关闭一段时间没有通信的”空闲“连接


# 客户/服务器程序设计范式

## 单进程

- accept+read/write
- 一次只能服务一个客户

## 多进程

- accept+fork,，每个客户一个子进程
  - 适合并发连接数不大的情况
  - 计算响应的工作量远大于fork()的开销
  - 适合长连接，不太适合短连接
- pre fork,TCP预先派生子进程服务器
  - accept无上锁保护
  - accept使用文件上锁保护
  - accept使用线程互斥锁上锁保护
  - 传递描述符


## 多线程

- accept+thread
- pre thread
  - 每个线程各自accept
  - 主线程统一accept

## Reactor模型变种

- thread-per-task
- worker thread
- thread Pool

## Reactors
- reactors in threads
- reactors in processes
- reactors + thread Pool

## 总结